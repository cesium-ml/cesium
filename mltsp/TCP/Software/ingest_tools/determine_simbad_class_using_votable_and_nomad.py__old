#!/usr/bin/env python
""" Determine SIMBAD science classifications, if available for the best matching SIMBAD source.

- Using SIMBAD VOTABLES to get classes
   - reference  kepler_find_simbad_public_sources.py when parsing votables
- Using distance cut when querying simbad votable.
     - distance cuts come from nomad / color distance code:  get_colors_for_tutor_sources.py
"""
import os, sys
import cPickle
import MySQLdb

try:
    from xml.etree import cElementTree as ElementTree # this is a nicer implementation
except:
    # This is caught in M45's python 2.4.3 distro where the elementtree module was installed instead
    from elementtree import ElementTree

sys.path.append(os.path.abspath(os.environ.get("TCP_DIR","") + \
              '/Software/feature_extract/Code/extractors')) # this for xmldict load only
import xmldict


class TCPDb():
    def connect_to_db(self):
        self.db = MySQLdb.connect(host=self.pars['mysql_hostname'],
                                  user=self.pars['mysql_username'],
                                  db=self.pars['mysql_database'],
                                  port=self.pars['mysql_port'])
        self.cursor = self.db.cursor()

class Determine_Simbad_Class(TCPDb):
    """ Determine SIMBAD science classifications, if available for the best matching SIMBAD source.
    """
    def __init__(self, pars={}):
        self.pars=pars
        self.connect_to_db()


    def make_nomad_simbad_source_match_dict(self):
        """
        """

        fp = open(self.pars['source_nomad_pkl_fpath'],'rb')
        nomad_sources = cPickle.load(fp)
        fp.close()

        srcid_class_match_dict = {}
        for src_id, src_dict in nomad_sources.iteritems():
            
            votable_fpath = "%s/%d.votable" % (self.pars['simbad_votable_dirpath'], src_id)
            
            votable_str = open(votable_fpath).read()
            if len(votable_str) < 310:
                #print "NO SIMBAD match:", src_id
                continue

            elemtree = ElementTree.fromstring(votable_str)
            xmld_data = xmldict.ConvertXmlToDict(elemtree)

            ### partially adapted from kepler_find_simbad_publid_sources.py:parse_class()
            ### - TODO: probably only need to do this once, if the VOTABLE format is consistant for all simbad sources
            b = xmld_data['VOTABLE']['RESOURCE']['TABLE']['FIELD']
            i_col_otype = -1
            i_col_dist = -1
            i_col_sptype = -1
            i_col_mainid = -1
            for i, elem in enumerate(b):
                if elem['name'] == 'OTYPE':
                    i_col_otype = i
                elif elem['name'] == 'DISTANCE':
                    i_col_dist = i
                elif elem['name'] == 'SP_TYPE':
                    i_col_sptype = i
                elif elem['name'] == 'MAIN_ID':
                    i_col_mainid = i
                    #break # this doesnt seem like it should work 100% of the time
            ###

            dist_list = []
            class_list = []
            sptype_list = []
            mainid_list = []
            if type(xmld_data['VOTABLE']['RESOURCE']['TABLE']['DATA']['TABLEDATA']['TR']) != type([]):
                dist_list.append(float(xmld_data['VOTABLE']['RESOURCE']['TABLE']['DATA']['TABLEDATA']['TR']['TD'][i_col_dist]))
                class_list.append(str(xmld_data['VOTABLE']['RESOURCE']['TABLE']['DATA']['TABLEDATA']['TR']['TD'][i_col_otype]))
                sp_type = str(xmld_data['VOTABLE']['RESOURCE']['TABLE']['DATA']['TABLEDATA']['TR']['TD'][i_col_sptype])
                if sp_type == "None":
                    sptype_list.append('')
                else:
                    sptype_list.append(sp_type)                    
                mainid = str(xmld_data['VOTABLE']['RESOURCE']['TABLE']['DATA']['TABLEDATA']['TR']['TD'][i_col_mainid])
                if mainid == "None":
                    mainid_list.append('')
                else:
                    mainid_list.append(mainid)
            else:
                for xmld in xmld_data['VOTABLE']['RESOURCE']['TABLE']['DATA']['TABLEDATA']['TR']:
                    dist_list.append(float(xmld['TD'][i_col_dist]))
                    class_list.append(str(xmld['TD'][i_col_otype]))
                    sp_type = str(xmld['TD'][i_col_otype])
                    if sp_type == "None":
                        sptype_list.append('')
                    else:
                        sptype_list.append(sp_type)
                    mainid = str(xmld['TD'][i_col_mainid])
                    if mainid == "None":
                        mainid_list.append('')
                    else:
                        mainid_list.append(mainid)

            if abs(dist_list[0] - nomad_sources[src_id]['dist']) <= 0.5:
                print "[0] matches: %d nomad=%lf deldist=%lf simbad=%s mainid=%s %s %s" % (src_id, nomad_sources[src_id]['dist'], abs(dist_list[0] - nomad_sources[src_id]['dist']), class_list[0], mainid_list[0], str(dist_list), str(class_list))
                srcid_class_match_dict[src_id] = {'class':class_list[0], 'simbad_dist':dist_list[0], 'simbad_sptype':sptype_list[0], 'main_id':mainid_list[0]}
                srcid_class_match_dict[src_id].update(nomad_sources[src_id])
            else:
                match_found = False
                for i in range(len(dist_list)):
                    if abs(dist_list[i] - nomad_sources[src_id]['dist']) <= 0.5:
                        print "[%d] matches: %d nomad=%lf simbad=%lf simbad=%s mainid=%s %s %s" % (i, src_id, nomad_sources[src_id]['dist'], dist_list[i], class_list[i], mainid_list[i], str(dist_list), str(class_list))
                        srcid_class_match_dict[src_id] = {'class':class_list[i], 'simbad_dist':dist_list[i], 'simbad_sptype':sptype_list[i], 'main_id':mainid_list[i]}
                        srcid_class_match_dict[src_id].update(nomad_sources[src_id])

                        match_found = True
                        break
                if not match_found:
                    print "NO dist match: %d nomad=%lf simbad=%s simbad=%s" % (src_id, nomad_sources[src_id]['dist'], str(dist_list), str(class_list))
                    
        return srcid_class_match_dict


    def insert_src_class_match_in_table(self, srcid_class_match_dict):
        """ Store the elements of the dictionary which matches simbad sources (and the sci classes)
        with nomad sources   (which match tutor sources).

        Store this in a mysql table.

        create table using:

        CREATE TABLE tutor_simbad_classes
        (src_id INT, simbad_class VARCHAR(16), simbad_dist FLOAT, simbad_sptype VARCHAR(16), main_id VARCHAR(32), PRIMARY KEY (src_id), INDEX (simbad_class));

        """
        insert_list = ["INSERT INTO tutor_simbad_classes (src_id, simbad_class, simbad_dist, simbad_sptype, main_id) VALUES "]

        for srcid, src_dict in srcid_class_match_dict.iteritems():
            insert_list.append("(%d, '%s', %f, '%s', '%s'), " % (srcid, src_dict['class'], src_dict['simbad_dist'], src_dict['simbad_sptype'], src_dict['main_id']))

        self.cursor.execute(''.join(insert_list)[:-2] + " ON DUPLICATE KEY UPDATE simbad_class=VALUES(simbad_class), simbad_dist=VALUES(simbad_dist), simbad_sptype=VALUES(simbad_sptype), main_id=VALUES(main_id)")

        import pdb; pdb.set_trace()
        print


    def get_simbad_abstracts(self, srcid_class_match_dict):
        """ Query and retrieve SIMBAD abstracts, using previously retrieved literature references for Simbad matched ASAS sources.
        """
        import urllib
        from BeautifulSoup import BeautifulSoup, Comment
        fp = open('/tmp/src_litrefs_1000.pkl','rb')
        src_litrefs = cPickle.load(fp)
        fp.close()

        for src_id, src_bib_dict in src_litrefs.iteritems():
            for bibcode, abstract_url in src_bib_dict.iteritems():
                f_url = urllib.urlopen(url_str)
                webpage_str = f_url.read()
                f_url.close()
                
                soup = BeautifulSoup(webpage_str)
                comments = soup.findAll(text=lambda text:isinstance(text, Comment))
                [comment.extract() for comment in comments]
                
                import pdb; pdb.set_trace()
                print




    def get_simbad_literature_refs(self, srcid_class_match_dict):
        """ Query and retrieve SIMBAD literature references for Simbad matched ASAS sources.
        """
        import urllib
        from BeautifulSoup import BeautifulSoup, Comment
        src_litrefs = {}
        srcid_list = srcid_class_match_dict.keys()
        srcid_list.sort()
        n_srcid = len(srcid_list)
        for i, src_id in enumerate(srcid_list):
            src_dict = srcid_class_match_dict[src_id]
            src_litrefs[src_id] = {}
            url_str = "http://adsabs.harvard.edu/cgi-bin/nph-abs_connect?db_key=AST&db_key=PRE&qform=AST&arxiv_sel=astro-ph&arxiv_sel=cond-mat&arxiv_sel=cs&arxiv_sel=gr-qc&arxiv_sel=hep-ex&arxiv_sel=hep-lat&arxiv_sel=hep-ph&arxiv_sel=hep-th&arxiv_sel=math&arxiv_sel=math-ph&arxiv_sel=nlin&arxiv_sel=nucl-ex&arxiv_sel=nucl-th&arxiv_sel=physics&arxiv_sel=quant-ph&arxiv_sel=q-bio&sim_query=YES&ned_query=YES&adsobj_query=YES&obj_req=YES&aut_logic=OR&obj_logic=OR&author=&object=%s&start_mon=&start_year=&end_mon=&end_year=&ttl_logic=OR&title=&txt_logic=OR&text=&nr_to_return=200&start_nr=1&jou_pick=ALL&ref_stems=&data_and=ALL&group_and=ALL&start_entry_day=&start_entry_mon=&start_entry_year=&end_entry_day=&end_entry_mon=&end_entry_year=&min_score=&sort=SCORE&data_type=SHORT&aut_syn=YES&ttl_syn=YES&txt_syn=YES&aut_wt=1.0&obj_wt=1.0&ttl_wt=0.3&txt_wt=3.0&aut_wgt=YES&obj_wgt=YES&ttl_wgt=YES&txt_wgt=YES&ttl_sco=YES&txt_sco=YES&version=1" % (src_dict['main_id'])
            f_url = urllib.urlopen(url_str)
            webpage_str = f_url.read()
            f_url.close()
            
            soup = BeautifulSoup(webpage_str)
            comments = soup.findAll(text=lambda text:isinstance(text, Comment))
            [comment.extract() for comment in comments]
            #print soup.html.body.form.find('table')
            #print '------------'
            #print soup.html.body.form.findAll('table')[1].table.tbody.findAll('tr')
            #soup.html.body.form.findAll('table')[1].extract()
            #bib_rows = soup.html.body.form.fetch('table')[1].fetch('tr')
            #print soup
            try:
                bib_rows = soup.html.body.form('table', limit=2)[1]('tr')
                print 'parsed:', i, n_srcid, src_id
            except:
                # likely no results returned
                #print 'len(soup.html.body.form.table):', len(soup.html.body.form.table)
                print 'skip:  ', i, n_srcid, src_id
                continue
            for r in bib_rows:
                for td in r('td'):
                    x = td.input
                    if x == None:
                        continue
                    bibcode = x['value']
                    abstract_url = td.a['href']
                    # NOTE: I could probably extract some author names, title
                    src_litrefs[src_id][bibcode] = abstract_url
                    #print bibcode, abstract_url
            #import pdb; pdb.set_trace()
            #print
            #fp = open('/tmp/124', 'w')
            #fp.write(webpage_str)
            #fp.close()
            #import pdb; pdb.set_trace()
            #print
            #elemtree = ElementTree.fromstring(webpage_str)
            #xmld_data = xmldict.ConvertXmlToDict(elemtree)
            #b = xmld_data['HTML']['body']['form']
            if (i % 500) == 0:
                fp = open('/tmp/src_litrefs_%d.pkl' % (i),'wb')
                cPickle.dump(src_litrefs,fp,1) # ,1) means a binary pkl is used.
                fp.close()


        import pdb; pdb.set_trace()
        print
        fp = open('/tmp/src_litrefs.pkl','wb')
        cPickle.dump(src_litrefs,fp,1) # ,1) means a binary pkl is used.
        fp.close()


    def main(self, do_insert=False):
        """
        """

        if not os.path.exists(self.pars['srcid_simbad_nomad_match_pkl_fpath']):
            srcid_class_match_dict = self.make_nomad_simbad_source_match_dict()
            fp = open(self.pars['srcid_simbad_nomad_match_pkl_fpath'],'wb')
            cPickle.dump(srcid_class_match_dict,fp,1) # ,1) means a binary pkl is used.
            fp.close()
        else:
            fp = open(self.pars['srcid_simbad_nomad_match_pkl_fpath'],'rb')
            srcid_class_match_dict = cPickle.load(fp)
            fp.close()
            
        if do_insert:
            self.insert_src_class_match_in_table(srcid_class_match_dict)
        
        #self.get_simbad_literature_refs(srcid_class_match_dict)
        self.get_simbad_abstracts(srcid_class_match_dict)

        import pdb; pdb.set_trace()
        print

    
if __name__ == '__main__':

    pars = { \
        'mysql_username':"pteluser", 
        'mysql_hostname':"192.168.1.25", 
        'mysql_database':'source_test_db', 
        'mysql_port':3306,
        'source_nomad_pkl_fpath':os.path.abspath(os.environ.get("TCP_DIR") + '/Data/best_nomad_src.pkl126'), #'/Data/best_nomad_src.pkl'), # generated by get_colors_for_tutor_sources.py
        'simbad_votable_dirpath':os.path.abspath(os.environ.get("HOME") + '/scratch/simbad_votables'),
        'srcid_simbad_nomad_match_pkl_fpath':os.path.abspath(os.environ.get("TCP_DIR") + '/Data/srcid_simbad_nomad_match.pkl'),
        }

    DetermineSimbadClass = Determine_Simbad_Class(pars=pars)
    DetermineSimbadClass.main()

#!/usr/bin/env python 
""" Tools for comparing the features generated by TCP for debosscher sources
found in the DotAstro.org database.  Comparison is with Debosscher's generated features
for the matching sources.


"""
import os, sys
import MySQLdb
from optparse import OptionParser
import cPickle
import gzip
import copy
import time
import pprint
import random
import feature_extraction_interface# NEEDED BY: self.get_src_obj_list() 
#                                               Feature_database()...

sys.path.append(os.path.abspath(os.environ.get("TCP_DIR") + \
              'Software/feature_extract/Code'))
import db_importer
sys.path.append(os.path.abspath(os.environ.get("TCP_DIR") + \
              'Software/feature_extract'))
from Code import *

import ingest_tools # NEEDED BY: generate_pairwise_classifications_for_perc_subset_lightcurves()



def parse_options():
    """ Deal with parsing command line options & --help.  Return options object.
    """
    parser = OptionParser(usage="usage: %prog cmd [options]")
    parser.add_option("-a","--debos_feat_compare",
                      dest="debos_feat_compare", \
                      action="store_true", default=False, \
                      help="Compare the values of TCP and Debosscher for a specific fature and sci-class")
    parser.add_option("-b","--debos_exact_freq_compare",
                      dest="debos_exact_freq_compare", \
                      action="store_true", default=False, \
                      help="Compare the values of TCP and Debosscher for a specific fature and sci-class")
    parser.add_option("-c","--freq_divergence_vs_percent_plot",
                      dest="freq_divergence_vs_percent_plot", \
                      action="store_true", default=False, \
                      help="Generate plots which show how difference of debosser freqs and TCP LS freqs differ over percentage sampling")
    parser.add_option("-d","--percent_sample__component_classifier_analysis",
                      dest="percent_sample__component_classifier_analysis", \
                      action="store_true", default=False, \
                      help="Analysis of feature, classification, vote confidence values for pairwise component classifiers applied to percent sampled debosscher data.")

    (options, args) = parser.parse_args()
    print "For help use flag:  --help" # KLUDGE since always: len(args) == 0
    return options


class Analysis_Deboss_TCP_Source_Compare:
    """ Main Analysis Class
    """
    def __init__(self, pars={}):
        self.pars = pars
        self.tcp_db = MySQLdb.connect(host=self.pars['tcp_hostname'], \
                                      user=self.pars['tcp_username'], \
                                      db=self.pars['tcp_database'], \
                                      port=self.pars['tcp_port'])
        self.tcp_cursor = self.tcp_db.cursor()


        self.tutor_db = MySQLdb.connect(host=self.pars['tcptutor_hostname'], \
                                 user=self.pars['tcptutor_username'], \
                                 passwd=self.pars['tcptutor_password'], \
                                 db=self.pars['tcptutor_database'], \
                                 port=self.pars['tcptutor_port'])
        self.tutor_cursor = self.tutor_db.cursor()


    def get_deboss_dotastro_source_lookup(self):
        """ Parsing Debosscher data files and querying DotAstro.org,
        determine which sources match and get related lookup info for each source.
        """
        ##### This gets a LC "filename" and its associated features/attributes
        deboss_attrib_dict = {}
        lines = open(self.pars['deboss_src_attribs_fpath']).readlines()
        for line in lines:
            line_list = line.split()
            fname = line_list[0]
            deboss_attrib_dict[fname] = {}
            for i, attrib in enumerate(self.pars['deboss_src_attribs_list'][1:]):
                deboss_attrib_dict[fname][attrib] = line_list[i+1]

        ##### This gets a source_name and all of the >=1 related lightcurve filenames:
        #    - NOTE: right now we just associate a single lightcurve per source_name since I assume . . .
        deboss_srcname_fname_lookup = {}
        lines = open(self.pars['deboss_src_datafile_lookup_fpath']).readlines()
        for line in lines:
            source_name = line[:23].strip()

            deboss_srcname_fname_lookup[source_name] = []

            filename_list = line[63:].split()
            for file_name in filename_list:
                file_name_sripped = file_name.strip()
                deboss_srcname_fname_lookup[source_name].append(file_name_sripped)

        debos_srcname_to_dotastro_srcid = {}
        dotastro_srcid_to_attribfiles = {}
        dotastro_srcid_to_debos_srcname = {}
        
        for source_name, fname_list in deboss_srcname_fname_lookup.iteritems():
            #if 'HIP 54413' in source_name:
            #    print 'yo'
            debos_srcname_to_dotastro_srcid[source_name] = []
            fname_match_found = False
            for filename in fname_list:
                select_str = 'SELECT source_id, source_name, project_id, class_id, pclass_id FROM sources WHERE project_id=122 and source_name = "%s"' % (source_name)
                self.tutor_cursor.execute(select_str)
                results = self.tutor_cursor.fetchall()
                if len(results) != 0:
                    if results[0][0] not in debos_srcname_to_dotastro_srcid[source_name]:
                        debos_srcname_to_dotastro_srcid[source_name].append(results[0][0])
                    if not dotastro_srcid_to_attribfiles.has_key(results[0][0]):
                        dotastro_srcid_to_attribfiles[results[0][0]] = []
                    dotastro_srcid_to_attribfiles[results[0][0]].append(filename)
                    if not dotastro_srcid_to_debos_srcname.has_key(results[0][0]):
                        dotastro_srcid_to_debos_srcname[results[0][0]] = []
                    if not source_name in dotastro_srcid_to_debos_srcname[results[0][0]]:
                        dotastro_srcid_to_debos_srcname[results[0][0]].append(source_name)
                    continue
                select_str = 'SELECT source_id, source_name, project_id, class_id, pclass_id FROM sources WHERE project_id=122 and source_name like "%s"' % (source_name.replace(' ','\_'))
                self.tutor_cursor.execute(select_str)
                results = self.tutor_cursor.fetchall()
                if len(results) != 0:
                    if results[0][0] not in debos_srcname_to_dotastro_srcid[source_name]:
                        debos_srcname_to_dotastro_srcid[source_name].append(results[0][0])
                    if not dotastro_srcid_to_attribfiles.has_key(results[0][0]):
                        dotastro_srcid_to_attribfiles[results[0][0]] = []
                    dotastro_srcid_to_attribfiles[results[0][0]].append(filename)
                    if not dotastro_srcid_to_debos_srcname.has_key(results[0][0]):
                        dotastro_srcid_to_debos_srcname[results[0][0]] = []
                    if not source_name in dotastro_srcid_to_debos_srcname[results[0][0]]:
                        dotastro_srcid_to_debos_srcname[results[0][0]].append(source_name)
                    continue
                select_str = 'SELECT source_id, source_name, project_id, class_id, pclass_id FROM sources WHERE project_id=122 and source_name = "%s"' % (filename)
                self.tutor_cursor.execute(select_str)
                results = self.tutor_cursor.fetchall()
                if len(results) != 0:
                    if results[0][0] not in debos_srcname_to_dotastro_srcid[source_name]:
                        debos_srcname_to_dotastro_srcid[source_name].append(results[0][0])
                    if not dotastro_srcid_to_attribfiles.has_key(results[0][0]):
                        dotastro_srcid_to_attribfiles[results[0][0]] = []
                    dotastro_srcid_to_attribfiles[results[0][0]].append(filename)
                    if not dotastro_srcid_to_debos_srcname.has_key(results[0][0]):
                        dotastro_srcid_to_debos_srcname[results[0][0]] = []
                    if not source_name in dotastro_srcid_to_debos_srcname[results[0][0]]:
                        dotastro_srcid_to_debos_srcname[results[0][0]].append(source_name)
                    continue
                select_str = 'SELECT source_id, source_name, project_id, class_id, pclass_id FROM sources WHERE project_id=122 and source_name = "%s"' % (source_name.replace(' ',''))
                self.tutor_cursor.execute(select_str)
                results = self.tutor_cursor.fetchall()
                if len(results) != 0:
                    if results[0][0] not in debos_srcname_to_dotastro_srcid[source_name]:
                        debos_srcname_to_dotastro_srcid[source_name].append(results[0][0])
                    if not dotastro_srcid_to_attribfiles.has_key(results[0][0]):
                        dotastro_srcid_to_attribfiles[results[0][0]] = []
                    dotastro_srcid_to_attribfiles[results[0][0]].append(filename)
                    if not dotastro_srcid_to_debos_srcname.has_key(results[0][0]):
                        dotastro_srcid_to_debos_srcname[results[0][0]] = []
                    if not source_name in dotastro_srcid_to_debos_srcname[results[0][0]]:
                        dotastro_srcid_to_debos_srcname[results[0][0]].append(source_name)
                    ###20100802 dstarr disables this since sometimes double classifications are given in Debos:
                    ###    and the other sci-class is entered as *_a into DotAstro: HIP54413, HIP54413_a
                    #continue
                select_str = 'SELECT source_id, source_name, project_id, class_id, pclass_id FROM sources WHERE project_id=122 and source_name like "%s\_' % (source_name.replace(' ','')) + '%"'
                self.tutor_cursor.execute(select_str)
                results = self.tutor_cursor.fetchall()
                if len(results) != 0:
                    if results[0][0] not in debos_srcname_to_dotastro_srcid[source_name]:
                        debos_srcname_to_dotastro_srcid[source_name].append(results[0][0])
                    if not dotastro_srcid_to_attribfiles.has_key(results[0][0]):
                        dotastro_srcid_to_attribfiles[results[0][0]] = []
                    dotastro_srcid_to_attribfiles[results[0][0]].append(filename)
                    if not dotastro_srcid_to_debos_srcname.has_key(results[0][0]):
                        dotastro_srcid_to_debos_srcname[results[0][0]] = []
                    if not source_name in dotastro_srcid_to_debos_srcname[results[0][0]]:
                        dotastro_srcid_to_debos_srcname[results[0][0]].append(source_name)
                    continue
        #for debos_srcname, dotastro_srcid_list in debos_srcname_to_dotastro_srcid.iteritems():
        #    if len(dotastro_srcid_list) > 1:
        #        print "len(dotastro_srcid_list) > 1:", debos_srcname, dotastro_srcid_list

        #for dotastro_srcid, debos_srcname in dotastro_srcid_to_debos_srcname.iteritems():
        #    print dotastro_srcid, debos_srcname
        
        dotastro_srcid_to_debos_attribs = {}
        for dotastro_srcid, attribfiles in dotastro_srcid_to_attribfiles.iteritems():
            matches_found = 0
            for attrib_file in attribfiles:
                if deboss_attrib_dict.has_key(attrib_file):
                    if os.path.exists("/home/pteluser/analysis/debosscher_20100707/TS-HIPPARCOS/" + attrib_file):
                        matches_found += 1
                        # I've checked that this only occurs once per dotastro sourceid (no multi LCs in there):
                        dotastro_srcid_to_debos_attribs[dotastro_srcid] = deboss_attrib_dict[attrib_file]
                    elif os.path.exists("/home/pteluser/analysis/debosscher_20100707/TS-OGLE/" + attrib_file):
                        matches_found += 1
                        # I've checked that this only occurs once per dotastro sourceid (no multi LCs in there):
                        dotastro_srcid_to_debos_attribs[dotastro_srcid] = deboss_attrib_dict[attrib_file]
            ##### DEBUG PRINTING:
            #if matches_found == 0:
            #    print     '  NO ATTRIB FILE:', dotastro_srcid, attribfiles
            #elif matches_found > 1:
            #    print     'MULTIPLE ATTRIBS:', dotastro_srcid, attribfiles
            #    for fname in attribfiles:
            #        pprint.pprint((fname, deboss_attrib_dict[fname]))
        return dotastro_srcid_to_debos_attribs


    def debos_exact_freq_compare(self):
        """ Try each debosscher found freq and see what the resulting phase-angle is.
              * If the phase angle is incorrect, then use the original calculation, or something else.
              * Nat thinks most likely the freqs have to match perfectly to get the same phase angles
        """
        if not os.path.exists(self.pars['srcid_debos_attribs_pkl_fpath']):
            srcid_to_debos_attribs = self.get_deboss_dotastro_source_lookup()
            fp = gzip.open(self.pars['srcid_debos_attribs_pkl_fpath'],'wb')
            cPickle.dump(srcid_to_debos_attribs,fp,1) # ,1) means a binary pkl is used.
            fp.close()
        else:
            fp = gzip.open(self.pars['srcid_debos_attribs_pkl_fpath'],'rb')
            srcid_to_debos_attribs = cPickle.load(fp)
            fp.close()

        class_shortname = 'alg'#
        #tcp_feat_name = 'freq%d_harmonics_rel_phase_1'
        #debos_feat_name = 'phi12'
        #tcp_feat_name = 'freq%d_harmonics_freq_0'
        debos_feat_name = 'f1'

        ### pairwise_pruned_dict is needed to get srcids for each sci-class
        fp=gzip.open(self.pars['trainset_pruned_pklgz_fpath'],'rb')
        pairwise_pruned_dict=cPickle.load(fp)
        fp.close()
        
        pairwise_pruned_dict[class_shortname]['srcid_list'].sort()
        for srcid in pairwise_pruned_dict[class_shortname]['srcid_list']:
            debos_freq1 = float(srcid_to_debos_attribs[int(srcid)]['f1'])
            tcp_srcid = int(srcid) + 100000000
            xml_fpath = "/home/pteluser/scratch/vosource_xml_writedir/%d.xml" % (tcp_srcid)

            d = mlens3.EventData(xml_fpath)

            ts_dict = {}
            for filter_name, elem_list in d.data['ts'].iteritems():
                ts_dict[filter_name] = {}
                for xml_elem in elem_list:
                    ts_dict[filter_name][xml_elem['name']] = xml_elem['val']


            print ### find matching vosource.xml fpath


        ### retrieve the mag[], merr[], time[] from (Joey-like) vosourcexml string
        ### generate LS attribs (like lightcurve.py:: get_out_dict(self, available, m, m_err))
        ### Then get debos PHA from float(srcid_to_debos_attribs[int(srcid)][debos_feat_name])
        ###   - and compare with newly found PHA

        #import pdb; pdb.set_trace()
        #print
        


    def freq_divergence_vs_percent_plot(self):
        """ Generate plot(s) which show how the average + errors of the abs(freq_debos_i - freq_tcp_i)
        change as different epochs sampling percentages are used.

        This method extracts feature values from  a .arff, whereas older functions:
            debos_exact_freq_compare() : gets features from xmls
            main()                     : gets features from RDB:feat_values.feat_val

        """
        # TODO: calculate freq differences from debosser freq values
        # TODO: generate plot
        import pairwise_classification
        pars = {'dotastro_arff_fpath':'/home/pteluser/scratch/deboss_percexclude__50k_26perc.arff',
                'arff_sciclass_dict_pkl_fpath':'/home/pteluser/deboss_percexclude__50k_26perc.pkl'}
        pc = pairwise_classification.Pairwise_Classification(pars=pars)
        ### NOTE: this generates a pkl file containing the arff_sciclass_dict structure:
        arff_sciclass_dict = pc.parse_arff(arff_has_ids=True, arff_has_classes=True, has_srcid=True, get_features=True)
        feat_id_name_dict = pc.feat_id_names
        print





    def main(self):
        """ This shows the defference between Deboss and TCP feature values, for a feature and sci-class
        """
        if not os.path.exists(self.pars['srcid_debos_attribs_pkl_fpath']):
            srcid_to_debos_attribs = self.get_deboss_dotastro_source_lookup()
            fp = gzip.open(self.pars['srcid_debos_attribs_pkl_fpath'],'wb')
            cPickle.dump(srcid_to_debos_attribs,fp,1) # ,1) means a binary pkl is used.
            fp.close()
        else:
            fp = gzip.open(self.pars['srcid_debos_attribs_pkl_fpath'],'rb')
            srcid_to_debos_attribs = cPickle.load(fp)
            fp.close()

        #import pdb; pdb.set_trace()
        """
        class_shortname = 'ds'#'rr-d'
        tcp_feat_name = 'freq1_harmonics_freq_0'
        debos_feat_name = 'f1'
        #feat_val_condition = '> 0'
        """

        class_shortname = 'alg'#
        #tcp_feat_name = 'freq%d_harmonics_rel_phase_1'
        #debos_feat_name = 'phi12'
        tcp_feat_name = 'freq%d_harmonics_freq_0'
        debos_feat_name = 'f1'

        fp=gzip.open(self.pars['trainset_pruned_pklgz_fpath'],'rb')
        pairwise_pruned_dict=cPickle.load(fp)
        fp.close()

        # select src_id, feat_values.feat_val, feat_lookup.feat_name, feat_lookup.filter_id from feat_values join feat_lookup ON (feat_values.feat_id=feat_lookup.feat_id and filter_id=8) where src_id=100149362 order by feat_name;
        pairwise_pruned_dict[class_shortname]['srcid_list'].sort()
        for srcid in pairwise_pruned_dict[class_shortname]['srcid_list']:
            debos_featval = float(srcid_to_debos_attribs[int(srcid)][debos_feat_name])
            tcp_srcid = int(srcid) + 100000000
            #select_str = 'SELECT src_id, feat_values.feat_val FROM feat_values join feat_lookup ON (feat_values.feat_id=feat_lookup.feat_id and filter_id=8) where src_id=%d AND feat_name="%s"' % (tcp_srcid, tcp_feat_name)
            select_str = 'SELECT src_id, feat_values.feat_val FROM feat_values join feat_lookup ON (feat_values.feat_id=feat_lookup.feat_id and filter_id=8) where src_id=%d AND feat_name="%s"' % (tcp_srcid,
                                                                                                                                                                                                    tcp_feat_name % (1))
            self.tcp_cursor.execute(select_str)
            results = self.tcp_cursor.fetchall()
            if len(results) == 0:
                freq1 = 0.
                continue
            else:
                freq1 = results[0][1]

            select_str = 'SELECT src_id, feat_values.feat_val FROM feat_values join feat_lookup ON (feat_values.feat_id=feat_lookup.feat_id and filter_id=8) where src_id=%d AND feat_name="%s"' % (tcp_srcid, 
                                                                                                                                                                                                    tcp_feat_name % (2))
            self.tcp_cursor.execute(select_str)
            results = self.tcp_cursor.fetchall()
            if len(results) == 0:
                freq2 = 0.
            else:
                freq2 = results[0][1]

            select_str = 'SELECT src_id, feat_values.feat_val FROM feat_values join feat_lookup ON (feat_values.feat_id=feat_lookup.feat_id and filter_id=8) where src_id=%d AND feat_name="%s"' % (tcp_srcid, 
                                                                                                                                                                                                    tcp_feat_name % (3))
            self.tcp_cursor.execute(select_str)
            results = self.tcp_cursor.fetchall()
            if len(results) == 0:
                freq3 = 0.
            else:
                freq3 = results[0][1]

            #print srcid, "%6.6f" % (debos_featval), freq1, freq2, freq3
            #"""
            if abs(freq1 - debos_featval) < 0.2:
                print srcid, "%6.6f" % (debos_featval), 'freq1', \
                      "%6.6f" % (freq1 - debos_featval), freq1, '\t', freq2, freq3
                
            elif abs(freq2 - debos_featval) < 0.2:
                print srcid, "%6.6f" % (debos_featval), 'freq2', \
                      "%6.6f" % (freq2 - debos_featval), freq2, '\t', freq1, freq3

            elif abs(freq3 - debos_featval) < 0.2:
                print srcid, "%6.6f" % (debos_featval), 'freq3', \
                      "%6.6f" % (freq3 - debos_featval), freq3, '\t', freq1, freq2

            else:
                #print srcid, "%6.6f" % (debos_featval), ' NONE         ', freq1, freq2, freq3
                print srcid, "%6.6f" % (debos_featval), ' NO_MATCH\t\t\t', freq1, freq2, freq3
            #"""
                

        # TODO: then query TCP-RDB for a particular feature range, to narrow the number of wources
        # TODO: then have & use some lookup table using debosscher data files


    def perc_subset_worker(self, src_id=None):
        """ IPython worker task to be called from:

        generate_pairwise_classifications_for_perc_subset_lightcurves()

        NOTE: This function now takes a contigous sampling of epochs.

        """
        xml_fpath = os.path.expandvars("$HOME/scratch/vosource_xml_writedir/%d.xml" % (src_id))


        signals_list = []
        gen_orig = generators_importers.from_xml(signals_list)
        gen_orig.signalgen = {}
        gen_orig.sig = db_importer.Source(xml_handle=xml_fpath, doplot=False, make_xml_if_given_dict=True)
        gen_orig.sdict = gen_orig.sig.x_sdict
        gen_orig.set_outputs() # this adds/fills self.signalgen[<filters>,multiband]{'input':{filled},'features':{empty},'inter':{empty}}

        signals_list_temp = []
        gen_temp = copy.deepcopy(gen_orig)

        #for perc in [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]:
        ###for perc in [0.1, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95, 1.0]:
        #for perc in [0.2, 0.4, 0.6, 0.65, 0.75, 0.85]:
        #for perc in [0.15, 0.25, 0.35, 0.45, 0.55, 0.925, 0.975, 0.875, 0.825, 0.775, 0.725, 0.675]:
        #for perc in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]:
        #for perc in [0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 1.0]:
        for perc in self.pars['subsample_percents_to_generate_xmls']:
            ### We generate several random, percent-subsampled vosource in order to include error info:
            for i in xrange(self.pars['num_percent_epoch_error_iterations']):
                new_srcid = "%d_%2.2f_%d" % (src_id, perc, i)
                new_fpath = os.path.expandvars("$HOME/scratch/xmls_deboss_percentage_exclude_2/%s.xml" % (new_srcid))

                ### NOTE: I make the assumption that we have already deleted these files, so I disable this:
                #if os.path.exists(new_fpath):
                #    continue

                dbi_src = db_importer.Source(make_dict_if_given_xml=False)
                for band, band_dict in gen_orig.sig.x_sdict['ts'].iteritems():
                    i_start = int(((len(band_dict['m'])+1) * (1 - perc)) * random.random())
                    i_end = i_start + int(perc * (len(band_dict['m'])+1))
                    gen_temp.sig.x_sdict['ts'][band]['m'] = band_dict['m'][i_start:i_end]
                    gen_temp.sig.x_sdict['ts'][band]['m_err'] = band_dict['m_err'][i_start:i_end]
                    gen_temp.sig.x_sdict['ts'][band]['t'] = band_dict['t'][i_start:i_end]
                dbi_src.source_dict_to_xml(gen_temp.sig.x_sdict)
                write_xml_str = dbi_src.xml_string

                signals_list = []
                gen = generators_importers.from_xml(signals_list)
                gen.generate(xml_handle=write_xml_str)
                gen.sig.add_features_to_xml_string(signals_list)                
                gen.sig.x_sdict['src_id'] = new_srcid
                dbi_src.source_dict_to_xml(gen.sig.x_sdict)

                fp = open(new_fpath, 'w')
                fp.write(dbi_src.xml_string)
                fp.close()
        return True


    def test_singlenode__perc_subset_lightcurves(self):
        """ A single node testing version of:
        generate_pairwise_classifications_for_perc_subset_lightcurves()
        """
        select_str = 'SELECT source_id FROM sources WHERE project_id=122'
        self.tutor_cursor.execute(select_str)
        results = self.tutor_cursor.fetchall()
        srcid_list = []
        for row in results:
            srcid_list.append(row[0] + 100000000)
    
        for src_id in srcid_list:
            # This generates a new xml file:
            out = self.perc_subset_worker(src_id)


    def generate_pairwise_classifications_for_perc_subset_lightcurves(self, do_multihost_ipython=False):
        """ For 10, 20, 30%...100% n_epochs of original deboscher source, we generate
        pairwise classifications and see how successful the pairwise classifier is at
        each partial sampling of a source.

        NOTE: The pairwise classifier has been trained using the full Debosscher dataset.

        NOTE: much of this is taken from ingest_tools.py:
        get_sources_using_xml_file_with_feature_extraction(srcid_uri, xml_fpath, only_sources_wo_features=0)
        """
        select_str = 'SELECT source_id FROM sources WHERE project_id=122'
        self.tutor_cursor.execute(select_str)
        results = self.tutor_cursor.fetchall()
        srcid_list = []
        for row in results:
            srcid_list.append(row[0] + 100000000)
            #srcid_list.append(int(row[0]))
        #import pdb; pdb.set_trace()

        try:
            from IPython.kernel import client
        except:
            pass

        self.mec = client.MultiEngineClient()
        self.mec.reset(targets=self.mec.get_ids()) # Reset the namespaces of all engines
        self.tc = client.TaskClient()
	self.task_id_list = []
        
        if do_multihost_ipython:
            scp_commands = """
from socket import gethostname
if gethostname() != 'tranx':
    os.system(os.path.expandvars("scp -c blowfish 192.168.1.25:/home/pteluser/analysis/debosscher_20100707/srcid_debos_attribs.pkl.gz $HOME/scratch/analysis_deboss_tcp_source_compare/"))
    os.system(os.path.expandvars("scp -c blowfish 192.168.1.25:/home/pteluser/Dropbox/work/WEKAj48_dotastro_ge1srcs_period_nonper__exclude_non_debosscher/pairwise_trainset__debosscher_table3.pkl.gz $HOME/scratch/analysis_deboss_tcp_source_compare/"))
    os.system(os.path.expandvars("scp -c blowfish 192.168.1.25:/home/pteluser/analysis/debosscher_20100707/list.dat $HOME/scratch/analysis_deboss_tcp_source_compare/"))
    os.system(os.path.expandvars("scp -c blowfish 192.168.1.25:/home/pteluser/analysis/debosscher_20100707/defatts.dat $HOME/scratch/analysis_deboss_tcp_source_compare/"))
            """
        else:
            scp_commands = ""

        mec_exec_str = """
import sys, os
sys.path.append(os.path.abspath(os.environ.get('TCP_DIR') + 'Software/ingest_tools'))
import analysis_deboss_tcp_source_compare
pars = {'num_percent_epoch_error_iterations':%d,
        'subsample_percents_to_generate_xmls':%s,
            'tcp_hostname':'192.168.1.25',
            'tcp_username':'pteluser',
            'tcp_port':     3306, 
            'tcp_database':'source_test_db',
            'tcptutor_hostname':'lyra.berkeley.edu',
            'tcptutor_username':'pteluser',
            'tcptutor_password':'Edwin_Hubble71',
            'tcptutor_port':     3306, # 13306,
            'tcptutor_database':'tutor',
            'srcid_debos_attribs_pkl_fpath':os.path.expandvars("$HOME/scratch/analysis_deboss_tcp_source_compare/srcid_debos_attribs.pkl.gz"),
            'trainset_pruned_pklgz_fpath':os.path.expandvars("$HOME/scratch/analysis_deboss_tcp_source_compare/pairwise_trainset__debosscher_table3.pkl.gz"),
            'deboss_src_datafile_lookup_fpath':os.path.expandvars("$HOME/scratch/analysis_deboss_tcp_source_compare/list.dat"),
            'deboss_src_attribs_fpath':os.path.expandvars("$HOME/scratch/analysis_deboss_tcp_source_compare/defatts.dat"),
            'deboss_src_attribs_list':['filename','f1','f2','f3','amp11','amp12','amp13','amp14','amp21','amp22','amp23','amp24','amp31','amp32','amp33','amp34','phi12','phi13','phi14','phi21','phi22','phi23','phi24','phi31','phi32','phi33','phi34','trend','varrat','varred','class'],
            }
AnalysisDebossTcpSourceCompare = analysis_deboss_tcp_source_compare.Analysis_Deboss_TCP_Source_Compare(pars=pars)
%s
for i in xrange(10):
    for j in xrange(9):
        os.system(os.path.expandvars("rm $HOME/scratch/xmls_deboss_percentage_exclude_2/*." + str(i) + "0_" + str(j) + ".xml"))
        os.system(os.path.expandvars("rm $HOME/scratch/xmls_deboss_percentage_exclude_2/*." + str(i) + "1_" + str(j) + ".xml"))
        os.system(os.path.expandvars("rm $HOME/scratch/xmls_deboss_percentage_exclude_2/*." + str(i) + "2_" + str(j) + ".xml"))
        os.system(os.path.expandvars("rm $HOME/scratch/xmls_deboss_percentage_exclude_2/*." + str(i) + "3_" + str(j) + ".xml"))
        os.system(os.path.expandvars("rm $HOME/scratch/xmls_deboss_percentage_exclude_2/*." + str(i) + "4_" + str(j) + ".xml"))
        os.system(os.path.expandvars("rm $HOME/scratch/xmls_deboss_percentage_exclude_2/*." + str(i) + "5_" + str(j) + ".xml"))
        os.system(os.path.expandvars("rm $HOME/scratch/xmls_deboss_percentage_exclude_2/*." + str(i) + "6_" + str(j) + ".xml"))
        os.system(os.path.expandvars("rm $HOME/scratch/xmls_deboss_percentage_exclude_2/*." + str(i) + "7_" + str(j) + ".xml"))
        os.system(os.path.expandvars("rm $HOME/scratch/xmls_deboss_percentage_exclude_2/*." + str(i) + "8_" + str(j) + ".xml"))
        os.system(os.path.expandvars("rm $HOME/scratch/xmls_deboss_percentage_exclude_2/*." + str(i) + "9_" + str(j) + ".xml"))
os.system(os.path.expandvars("rm $HOME/scratch/xmls_deboss_percentage_exclude_2/*.xml"))


        """ % (self.pars['num_percent_epoch_error_iterations'],
               str(self.pars['subsample_percents_to_generate_xmls']),
               scp_commands)

        self.mec.execute(mec_exec_str)
	time.sleep(10) # This may be needed.

        for src_id in srcid_list:
            #if 1:
            #    ##### Testing / debugging only:
            #    out = self.perc_subset_worker(src_id)
            #    import pdb; pdb.set_trace()
            #else:
            exec_str = "out = AnalysisDebossTcpSourceCompare.perc_subset_worker(src_id=%d)" % (src_id)
            taskid = self.tc.run(client.StringTask(exec_str, pull='out', retries=3))
            self.task_id_list.append(taskid)

	while ((self.tc.queue_status()['scheduled'] > 0) or
 	       (self.tc.queue_status()['pending'] > 0)):
            tasks_to_pop = []
	    for task_id in self.task_id_list:
	        temp = self.tc.get_task_result(task_id, block=False)
                if temp == None:
                    continue
                temp2 = temp.results
                if temp2 == None:
                    continue
                results = temp2.get('out',None)
                if results == None:
                    continue # skip these sources (I think generally UNKNOWN ... science classes)
                out_dict = results
                if out_dict == True:
		    tasks_to_pop.append(task_id)
	    for task_id in tasks_to_pop:
	        self.task_id_list.remove(task_id)
            print self.tc.queue_status()
            print 'Sleep... 20  in generate_pairwise_classifications_for_perc_subset_lightcurves()'
	    time.sleep(20)


        # Now lets scp the results from non-tranx IPython workers:
        exec_str = """
import time
pid = str(os.getpid())
os.system('echo "' + pid + '" > /tmp/pairwise_pid.pid')
time.sleep(2)
pid_read = open('/tmp/pairwise_pid.pid').read().strip()
if (pid == pid_read) and (gethostname() != 'tranx'):
    os.system("tar -C %s -cf %s/analysis_transfer_" + gethostname() + ".tar xmls_deboss_percentage_exclude_2")
    os.system("scp -C -c blowfish %s/analysis_transfer_" + gethostname() + ".tar pteluser@192.168.1.25:/home/pteluser/scratch/")
    os.system("ssh -n pteluser@192.168.1.25 tar -C ~/scratch -xvf ~/scratch/analysis_transfer_" + gethostname() + ".tar")
    os.system("ssh -n pteluser@192.168.1.25 rm ~/scratch/analysis_transfer_" + gethostname() + ".tar")

        """ % (os.path.expandvars("$HOME/scratch"),
               os.path.expandvars("$HOME/scratch"),
               os.path.expandvars("$HOME/scratch"))
        self.mec.execute(exec_str)
	time.sleep(2) # This may be needed.


    def percent_sample__component_classifier_analysis(self):
        """ Analysis of feature, classification, vote confidence values for pairwise component classifiers applied to percent sampled debosscher data.

        TODO: Plot: for each sciclass, Y:<cumulative conf when orig classified>,
                                                 X:<percent contiguous sampled>
               -> expect to see cumulative percent drop as percent gets smaller, especially for some classes
               -> class_summary[<class>]:{
                                 'x_perc':[0.7,0.8,0.9,..],
                                 'y_avg_conf':[0.9,1.0,0.8,..],
                                 'perc_conf_lists':{'perc':[0.9,0.7,...] # for all corr classif cases
                                                    'conf':[1.0,1.0,..]  # for all corr classif cases

        TODO: Plot: for each feature, Y:<cumulat conf of classifier which uses feat, when corr classified>,
                                                 X:<percent contiguous sampled>
               -> expect to see cumulative percent drop as percent gets smaller, especially for some classes
               -> this will require parsing 

        """
        from scipy import stats
        import matplotlib
        import matplotlib.pyplot as plt
        ### This is taken from pairwise_classification.py (__main__):
        self.pars.update({'plot_symb':['o','s','v','d','<'], # ,'+','x','.', ,'>','^'
                          'feat_distrib_colors':['#000000',
                                   '#ff3366',
                                   '#660000',
                                   '#aa0000',
                                   '#ff0000',
                                   '#ff6600',
                                   '#996600',
                                   '#cc9900',
                                   '#ffff00',
                                   '#ffcc33',
                                   '#ffff99',
                                   '#99ff99',
                                   '#666600',
                                   '#99cc00',
                                   '#00cc00',
                                   '#006600',
                                   '#339966',
                                   '#33ff99',
                                   '#006666',
                                   '#66ffff',
                                   '#0066ff',
                                   '#0000cc',
                                   '#660099',
                                   '#993366',
                                   '#ff99ff',
                                   '#440044'],
                          })



        fpath = '/home/pteluser/scratch/classifier_confidence_analysis.pkl.gz'

        fp=gzip.open(fpath,'rb')
        big_dict=cPickle.load(fp)
        fp.close()

        class_summary = {}
        for class_name in big_dict.values()[0][0]['confusion_matrix_index_list']:
            class_summary[class_name] = {'x_perc':[],
                                         'y_avg_conf':[],
                                         'perc_conf_lists':{'perc':[],
                                                            'conf':[]}}
        for perc_str, perc_dict in big_dict.iteritems():
            perc_flt = float(perc_str)
            for set_num, set_dict in perc_dict.iteritems():
                for classif_name, classif_dict in set_dict['confids'].iteritems():
                    # NOTE: classif_dict{'class_pred':[u'cm', u'cm', u'cm']
                    #                    'orig_class':['cm', 'cm', 'cm']
                    #                    'class_conf':[1.0, 1.0, 1.0]
                    for i, orig_class in enumerate(classif_dict['orig_class']):
                        if orig_class == classif_dict['class_pred'][i]:
                            class_summary[orig_class]['perc_conf_lists']['perc'].append(perc_flt)
                            class_summary[orig_class]['perc_conf_lists']['conf'].append(classif_dict['class_conf'][i])

        # TODO: make some plots using the colors / symbols ive been using in other plots
        # todo: I need to normalize by the number of sources in a scienc class


        matplotlib.rcParams['axes.unicode_minus'] = False
        matplotlib.rcParams['legend.fontsize'] = 7
        fig = plt.figure(figsize=(9,11), dpi=100)
        ax = fig.add_subplot(1, 1, 1) # (n_row, n_cols, i_feat + 1)

        perc_list = []
        perc_list.sort()
        for perc in big_dict.keys():
            perc_list.append(float(perc))
        perc_list.sort()
        for i_class, class_name in enumerate(big_dict.values()[0][0]['confusion_matrix_index_list']):
            perc_summary_dict = {}
            for perc_flt in perc_list:
                perc_summary_dict[perc_flt] = []
            for i, perc in enumerate(class_summary[class_name]['perc_conf_lists']['perc']):
                perc_summary_dict[perc].append(class_summary[class_name]['perc_conf_lists']['conf'][i])
            conf_mean_list = []
            conf_error_list = []
            for perc_flt in perc_list:
                conf_mean_list.append(stats.mean(perc_summary_dict[perc_flt]))
                conf_error_list.append(stats.std(perc_summary_dict[perc_flt]))
                
            #ax.plot(perc_list, conf_mean_list, conf_error_list,
            #                color=self.pars['feat_distrib_colors'][i_class+1],
            #                linestyle='solid', linewidth=2,
            #        marker=self.pars['plot_symb'][i_class % len(self.pars['plot_symb'])], markersize=4)
            ax.plot(perc_list, conf_mean_list, 
                            color=self.pars['feat_distrib_colors'][i_class+1],
                            linestyle='solid', linewidth=2)
            ax.errorbar(perc_list, conf_mean_list, yerr=conf_error_list,
                        c=self.pars['feat_distrib_colors'][i_class+1], label=class_name, linewidth=5,
                        elinewidth=1,
                        marker=self.pars['plot_symb'][i_class % len(self.pars['plot_symb'])], markersize=4)

        ax.set_ylim(0.98, 1.0)
        ax.set_xlim(min(perc_list) - 0.01, max(perc_list) + 0.1)
        #ax.set_yticks(y_tick_vals)
        #ax.set_yticklabels(y_labels)
        #ax.set_xticks([])
        #ax.set_xticklabels([])
        ax.set_title("Correctly classified confidences vs % sampling", fontsize=12)
        ax.set_xlabel("Percent of epochs used")
        ax.set_ylabel("Avg pairwise classifier confidence")
        ax.legend()

        fpath = "/home/pteluser/scratch/plot_confids_vs_percent.png"
        if os.path.exists(fpath):
            os.system('rm ' + fpath)
        plt.savefig(fpath)
        os.system("cp %s /home/pteluser/Dropbox/" % (fpath))


    def percent_sample__component_classifier_analysis__per_classifier(self):
        """ Analysis of feature, classification, vote confidence values for pairwise component classifiers applied to percent sampled debosscher data.

        TODO: Plot: for each sciclass, Y:<cumulative conf when orig classified>,
                                                 X:<percent contiguous sampled>
               -> expect to see cumulative percent drop as percent gets smaller, especially for some classes
               -> class_summary[<class>]:{
                                 'x_perc':[0.7,0.8,0.9,..],
                                 'y_avg_conf':[0.9,1.0,0.8,..],
                                 'perc_conf_lists':{'perc':[0.9,0.7,...] # for all corr classif cases
                                                    'conf':[1.0,1.0,..]  # for all corr classif cases

        TODO: Plot: for each feature, Y:<cumulat conf of classifier which uses feat, when corr classified>,
                                                 X:<percent contiguous sampled>
               -> expect to see cumulative percent drop as percent gets smaller, especially for some classes
               -> this will require parsing 

        """
        from scipy import stats
        import numpy
        import matplotlib
        import matplotlib.pyplot as plt
        ### This is taken from pairwise_classification.py (__main__):
        self.pars.update({'plot_symb':['o','s','v','d','<'], # ,'+','x','.', ,'>','^'
                          'feat_distrib_colors':['#000000',
                                   '#ff3366',
                                   '#660000',
                                   '#aa0000',
                                   '#ff0000',
                                   '#ff6600',
                                   '#996600',
                                   '#cc9900',
                                   '#ffff00',
                                   '#ffcc33',
                                   '#ffff99',
                                   '#99ff99',
                                   '#666600',
                                   '#99cc00',
                                   '#00cc00',
                                   '#006600',
                                   '#339966',
                                   '#33ff99',
                                   '#006666',
                                   '#66ffff',
                                   '#0066ff',
                                   '#0000cc',
                                   '#660099',
                                   '#993366',
                                   '#ff99ff',
                                   '#440044'],
                          })


        fpath = '/home/pteluser/scratch/classifier_confidence_analysis.pkl.gz'
        # DEBUG only:
        #fpath = '/media/raid_0/debosscher_classification_analysis/20100918a/classifier_confidence_analysis.pkl.gz'

        fp=gzip.open(fpath,'rb')
        big_dict=cPickle.load(fp)
        fp.close()

        class_summary = {}
        for classifier_name in big_dict.values()[0].values()[0]['confids'].keys():
            class_summary[classifier_name] = {}
            classnames = classifier_name.split(';')
            for class_name in classnames:
                class_summary[classifier_name][class_name] = {'perc_conf_lists':{'perc':[],
                                                                                 'conf':[]}}

        for perc_str, perc_dict in big_dict.iteritems():
            perc_flt = float(perc_str)
            for set_num, set_dict in perc_dict.iteritems():
                for classifier_name, classif_dict in set_dict['confids'].iteritems():
                    # NOTE: classif_dict{'class_pred':[u'cm', u'cm', u'cm']  len() = 1380
                    #                    'orig_class':['cm', 'cm', 'cm']     len() = 1380
                    #                    'class_conf':[1.0, 1.0, 1.0]        len() = 1380
                    classnames = classifier_name.split(';')
                    for i, orig_class in enumerate(classif_dict['orig_class']):
                        if orig_class == classif_dict['class_pred'][i]:
                            # This stores seperately each of the pair of classes:
                            class_summary[classifier_name][orig_class]['perc_conf_lists']['perc'].append(perc_flt)
                            class_summary[classifier_name][orig_class]['perc_conf_lists']['conf'].append(classif_dict['class_conf'][i])

        perc_list = []
        perc_list.sort()
        for perc in big_dict.keys():
            perc_list.append(float(perc))
        perc_list.sort()
        #for i_class, class_name in enumerate(big_dict.values()[0][0]['confusion_matrix_index_list']):
        class_summary_keys = class_summary.keys()
        class_summary_keys.sort()

        i_class = 0
        sorted_class_name_list = big_dict.values()[0][0]['confusion_matrix_index_list']

        #import pdb; pdb.set_trace()
        #print
        for classname_0 in sorted_class_name_list:
            classname_1_dict = {}
            for i, classifier_name in enumerate(class_summary_keys):
                component_classes = classifier_name.split(';')
                if classname_0 in component_classes:
                    classname_1 = classifier_name.strip(classname_0).strip(';')
                    classname_1_dict[classname_1] = classifier_name

            matplotlib.rcParams['axes.unicode_minus'] = False
            matplotlib.rcParams['legend.fontsize'] = 7
            fig = plt.figure(figsize=(9,11), dpi=100)
            ax = fig.add_subplot(1, 1, 1) # (n_row, n_cols, i_feat + 1)

            for i_class, classname_1 in enumerate(sorted_class_name_list):
                if not classname_1_dict.has_key(classname_1):
                    continue
                classifier_name = classname_1_dict[classname_1]

                for a_class in class_summary[classifier_name].keys():
                
                    perc_summary_dict = {}
                    for perc_flt in perc_list:
                        perc_summary_dict[perc_flt] = []
                    for i, perc in enumerate(class_summary[classifier_name][a_class]['perc_conf_lists']['perc']):
                        perc_summary_dict[perc].append(class_summary[classifier_name][a_class]['perc_conf_lists']['conf'][i])
                    conf_mean_list = []
                    conf_error_list = []
                    for perc_flt in perc_list:
                        conf_mean_list.append(stats.mean(perc_summary_dict[perc_flt]))
                        conf_error_list.append(stats.std(perc_summary_dict[perc_flt]))
                        
                    if a_class == classname_0:
                        #ax.plot(perc_list, conf_mean_list, 
                        #            color=self.pars['feat_distrib_colors'][i_class+1],
                        #            linestyle='solid', linewidth=2)
                        ax.errorbar(perc_list, conf_mean_list,
                                c=self.pars['feat_distrib_colors'][i_class+1], linewidth=4,
                                elinewidth=1, linestyle='--',
                                marker=self.pars['plot_symb'][i_class % len(self.pars['plot_symb'])], markersize=4)
                    else:
                        #ax.plot(perc_list, conf_mean_list, 
                        #            color=self.pars['feat_distrib_colors'][i_class+1],
                        #            linestyle='dashed', linewidth=2)
                        ax.errorbar(perc_list, conf_mean_list, label=classifier_name, 
                                c=self.pars['feat_distrib_colors'][i_class+1], linewidth=4,
                                elinewidth=1, 
                                marker=self.pars['plot_symb'][i_class % len(self.pars['plot_symb'])], markersize=4)
                        #ax.errorbar(perc_list, 1 - numpy.array(conf_mean_list), yerr=conf_error_list,
                        #        c=self.pars['feat_distrib_colors'][i_class+1], linewidth=5,
                        #        elinewidth=1, linestyle='--',
                        #        marker=self.pars['plot_symb'][i_class % len(self.pars['plot_symb'])], markersize=4)

            ax.set_ylim(0.80, 1.0) # ax.set_ylim(0.85, 1.0)
            #ax.set_yscale('log')
            ax.set_xlim(min(perc_list) - 0.01, max(perc_list) + 0.1)
            ax.set_xticks(numpy.arange(0.1, 1.1, 0.1))

            ax.set_title("%s: Correctly classified confidences vs sampling percent"% (classname_0), fontsize=12)
            ax.set_xlabel("Percent of epochs used")
            ax.set_ylabel("Avg pairwise classifier confidence")
            ax.legend(loc=3, ncol=2, columnspacing=1)

            fpath = "/home/pteluser/scratch/plot_confids_vs_percent__per_classifier_final_%s.png" % (classname_0)
            if os.path.exists(fpath):
                os.system('rm ' + fpath)
            plt.savefig(fpath)
            os.system("cp %s /home/pteluser/Dropbox/" % (fpath))
            plt.cla()
            plt.clf()




if __name__ == '__main__':
    options = parse_options()
    #'dotastro_arff_fpath':os.path.expandvars('$HOME/scratch/dotastro_sources_3src_or_more_20091123.arff'),
    #'dotastro_arff_fpath':os.path.expandvars('$HOME/Dropbox/work/20100524_xmlarffs_4312_includes_nonfold_3src_nocomboband_added_2percentile/train_output_20100517_dotastro_xml_with_features_removed_sdss.arff'),

    #'subsample_percents_to_generate_xmls':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.0],  # This takes 22 cores about 1 hour, at 'num_percent_epoch_error_iterations':1
    #'subsample_percents_to_generate_xmls':[0.1, 0.2, 0.3, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.72, 0.74, 0.76, 0.78, 0.8, 0.82, 0.84, 0.86, 0.88, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.0], # This takes 22 cores ??? considering that tranx will do a second round (count=22 + 8)  !!!THIS IS TOO MUCH!!!   # # # #TODO: 16perc * 9sets is much more reasonable, memory/resource/ipython-node wise

    pars = {'num_percent_epoch_error_iterations':2, # !!! NOTE: This must be the same in pairwise_classification.py:pars[]
            'subsample_percents_to_generate_xmls':[0.1], # This takes 22 cores ??? considering that tranx will do a second round (count=22 + 8)  # # # #TODO: 16perc * 9sets is much more reasonable, memory/resource/ipython-node wise
            'tcp_hostname':'192.168.1.25',
            'tcp_username':'pteluser',
            'tcp_port':     3306, 
            'tcp_database':'source_test_db',
            'tcptutor_hostname':'lyra.berkeley.edu',
            'tcptutor_username':'pteluser',
            'tcptutor_password':'Edwin_Hubble71',
            'tcptutor_port':     3306, # 13306,
            'tcptutor_database':'tutor',
            'srcid_debos_attribs_pkl_fpath':'/home/pteluser/analysis/debosscher_20100707/srcid_debos_attribs.pkl.gz',
            #OBSOLETE#'arff_fpath':os.path.expandvars(os.path.expanduser("~/scratch/dotastro_ge1srcs_period_nonper__exclude_non_debosscher.arff")),
            'trainset_pruned_pklgz_fpath':"/home/pteluser/Dropbox/work/WEKAj48_dotastro_ge1srcs_period_nonper__exclude_non_debosscher/pairwise_trainset__debosscher_table3.pkl.gz",
            'deboss_src_datafile_lookup_fpath':'/home/pteluser/analysis/debosscher_20100707/list.dat',
            'deboss_src_attribs_fpath':'/home/pteluser/analysis/debosscher_20100707/defatts.dat',
            'deboss_src_attribs_list':['filename','f1','f2','f3','amp11','amp12','amp13','amp14','amp21','amp22','amp23','amp24','amp31','amp32','amp33','amp34','phi12','phi13','phi14','phi21','phi22','phi23','phi24','phi31','phi32','phi33','phi34','trend','varrat','varred','class'],
            }



    AnalysisDebossTcpSourceCompare = Analysis_Deboss_TCP_Source_Compare(pars=pars)
    if options.debos_feat_compare:
        ##### This shows the defference between Deboss and TCP feature values, for a feature and sci-class:
        AnalysisDebossTcpSourceCompare.main()
    elif options.debos_exact_freq_compare:
        AnalysisDebossTcpSourceCompare.debos_exact_freq_compare()
    elif options.freq_divergence_vs_percent_plot:
        AnalysisDebossTcpSourceCompare.freq_divergence_vs_percent_plot()
    elif options.percent_sample__component_classifier_analysis:
        AnalysisDebossTcpSourceCompare.percent_sample__component_classifier_analysis__per_classifier()
        #AnalysisDebossTcpSourceCompare.percent_sample__component_classifier_analysis()
    else:
        ### For testing:
        #AnalysisDebossTcpSourceCompare.test_singlenode__perc_subset_lightcurves()
        #sys.exit()
        ### Normal non-testing IPython mode:
        AnalysisDebossTcpSourceCompare.generate_pairwise_classifications_for_perc_subset_lightcurves( \
                                                        do_multihost_ipython=True)
